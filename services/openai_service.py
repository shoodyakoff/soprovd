"""
–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å OpenAI API
"""
import asyncio
import logging
from typing import Optional
from openai import AsyncOpenAI
from config import (
    OPENAI_API_KEY, 
    OPENAI_MODEL, 
    OPENAI_FALLBACK_MODEL,
    OPENAI_TIMEOUT,
    MAX_GENERATION_ATTEMPTS,
    MIN_RESPONSE_LENGTH,
    OPENAI_TEMPERATURE,
    OPENAI_TOP_P,
    OPENAI_PRESENCE_PENALTY,
    OPENAI_FREQUENCY_PENALTY
)
from utils.prompts import get_cover_letter_prompt

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logger = logging.getLogger(__name__)

class OpenAIService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å OpenAI API"""
    
    def __init__(self):
        self.client = AsyncOpenAI(api_key=OPENAI_API_KEY)
    
    async def test_api_connection(self) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–∞–±–æ—Ç—É OpenAI API
        
        Returns:
            True –µ—Å–ª–∏ API —Ä–∞–±–æ—Ç–∞–µ—Ç, False –≤ –ø—Ä–æ—Ç–∏–≤–Ω–æ–º —Å–ª—É—á–∞–µ
        """
        try:
            logger.info("–ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ OpenAI API...")
            
            response = await asyncio.wait_for(
                self.client.chat.completions.create(
                    model=OPENAI_MODEL,
                    messages=[
                        {
                            "role": "user",
                            "content": "–ü—Ä–∏–≤–µ—Ç! –≠—Ç–æ —Ç–µ—Å—Ç. –û—Ç–≤–µ—Ç—å –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º: —Ä–∞–±–æ—Ç–∞—é"
                        }
                    ],
                    max_tokens=10,
                    temperature=0.1
                ),
                timeout=30
            )
            
            if response.choices and response.choices[0].message.content:
                logger.info("‚úÖ OpenAI API —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–æ—Ä–º–∞–ª—å–Ω–æ")
                return True
            else:
                logger.error("‚ùå OpenAI API –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç")
                return False
                
        except asyncio.TimeoutError:
            logger.error("‚ùå –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ OpenAI API")
            return False
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ OpenAI API: {e}")
            
            # –ü–æ–ø—Ä–æ–±—É–µ–º fallback –º–æ–¥–µ–ª—å
            try:
                logger.info("–ü—Ä–æ–±—É–µ–º fallback –º–æ–¥–µ–ª—å...")
                response = await asyncio.wait_for(
                    self.client.chat.completions.create(
                        model=OPENAI_FALLBACK_MODEL,
                        messages=[
                            {
                                "role": "user",
                                "content": "–¢–µ—Å—Ç"
                            }
                        ],
                        max_tokens=5,
                        temperature=0.1
                    ),
                    timeout=30
                )
                
                if response.choices and response.choices[0].message.content:
                    logger.info(f"‚úÖ Fallback –º–æ–¥–µ–ª—å {OPENAI_FALLBACK_MODEL} —Ä–∞–±–æ—Ç–∞–µ—Ç")
                    return True
                else:
                    logger.error("‚ùå Fallback –º–æ–¥–µ–ª—å –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç")
                    return False
                    
            except Exception as fallback_e:
                logger.error(f"‚ùå Fallback –º–æ–¥–µ–ª—å —Ç–æ–∂–µ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç: {fallback_e}")
                return False

    async def generate_cover_letter(
        self, 
        job_description: str, 
        resume: str, 
        style: str
    ) -> Optional[str]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–æ–ø—Ä–æ–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ–µ –ø–∏—Å—å–º–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º OpenAI API
        
        Args:
            job_description: –û–ø–∏—Å–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏
            resume: –†–µ–∑—é–º–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞
            style: –°—Ç–∏–ª—å –ø–∏—Å—å–º–∞ (neutral, bold, formal)
            
        Returns:
            –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø–∏—Å—å–º–æ –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        """
        
        prompt = get_cover_letter_prompt(job_description, resume, style)
        
        # –ü—Ä–æ–±—É–µ–º —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–∏—Å—å–º–æ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏
        for attempt in range(MAX_GENERATION_ATTEMPTS):
            try:
                logger.info(f"–ü–æ–ø—ã—Ç–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ #{attempt + 1} (temp={OPENAI_TEMPERATURE}, top_p={OPENAI_TOP_P}, presence={OPENAI_PRESENCE_PENALTY}, frequency={OPENAI_FREQUENCY_PENALTY})")
                
                response = await asyncio.wait_for(
                    self._make_openai_request(prompt),
                    timeout=OPENAI_TIMEOUT
                )
                
                if response and self._is_response_complete(response):
                    logger.info("–£—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –ø–∏—Å—å–º–æ")
                    return response
                else:
                    # –õ–æ–≥–∏—Ä—É–µ–º –¥–µ—Ç–∞–ª–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                    response_length = len(response) if response else 0
                    response_preview = response[:200] if response else "None"
                    logger.warning(f"–ù–µ–ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ #{attempt + 1}. –î–ª–∏–Ω–∞: {response_length}, –ü—Ä–µ–≤—å—é: {response_preview}")
                    if response:
                        ends_properly = response.strip().endswith(('.', '!', '?'))
                        logger.warning(f"–ó–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ: {ends_properly}, –ü–æ—Å–ª–µ–¥–Ω–∏–µ 50 —Å–∏–º–≤–æ–ª–æ–≤: '{response[-50:]}'")
                    
            except asyncio.TimeoutError:
                logger.error(f"–¢–∞–π–º–∞—É—Ç –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ #{attempt + 1}")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ #{attempt + 1}: {e}")
        
        logger.error("–í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–µ—É—Å–ø–µ—à–Ω—ã")
        return None
    
    async def _make_openai_request(self, prompt: str) -> Optional[str]:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –∑–∞–ø—Ä–æ—Å –∫ OpenAI API
        
        Args:
            prompt: –ü—Ä–æ–º–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            
        Returns:
            –û—Ç–≤–µ—Ç –æ—Ç OpenAI –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        """
        try:
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –æ—Å–Ω–æ–≤–Ω—É—é –º–æ–¥–µ–ª—å
            response = await self.client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=[
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                max_tokens=1500,
                temperature=OPENAI_TEMPERATURE,
                top_p=OPENAI_TOP_P,
                presence_penalty=OPENAI_PRESENCE_PENALTY,
                frequency_penalty=OPENAI_FREQUENCY_PENALTY
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ —Å –º–æ–¥–µ–ª—å—é {OPENAI_MODEL}: {e}")
            
            # –ü—Ä–æ–±—É–µ–º fallback –º–æ–¥–µ–ª—å
            try:
                response = await self.client.chat.completions.create(
                    model=OPENAI_FALLBACK_MODEL,
                    messages=[
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    max_tokens=1500,
                    temperature=OPENAI_TEMPERATURE,
                    top_p=OPENAI_TOP_P,
                    presence_penalty=OPENAI_PRESENCE_PENALTY,
                    frequency_penalty=OPENAI_FREQUENCY_PENALTY
                )
                
                return response.choices[0].message.content
                
            except Exception as fallback_e:
                logger.error(f"–û—à–∏–±–∫–∞ —Å fallback –º–æ–¥–µ–ª—å—é {OPENAI_FALLBACK_MODEL}: {fallback_e}")
                return None
    
    def _is_response_complete(self, response: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –æ—Ç–≤–µ—Ç –ø–æ–ª–Ω—ã–º
        
        Args:
            response: –û—Ç–≤–µ—Ç –æ—Ç OpenAI
            
        Returns:
            True –µ—Å–ª–∏ –æ—Ç–≤–µ—Ç –∫–∞–∂–µ—Ç—Å—è –ø–æ–ª–Ω—ã–º
        """
        if not response or len(response.strip()) < MIN_RESPONSE_LENGTH:
            logger.warning(f"–û—Ç–≤–µ—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π: {len(response.strip()) if response else 0} < {MIN_RESPONSE_LENGTH}")
            return False
        
        response = response.strip()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –æ–∫–æ–Ω—á–∞–Ω–∏—è –ø–∏—Å—å–º–∞
        valid_endings = [
            # –û–±—ã—á–Ω—ã–µ –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
            '.', '!', '?',
            # –ü–æ–¥–ø–∏—Å–∏ –≤ –ø–∏—Å—å–º–∞—Ö
            '[–í–∞—à–µ –∏–º—è]', '[–í–∞—à–µ –ò–º—è]', '[–ò–º—è]', '[–≤–∞—à–µ –∏–º—è]',
            # –î—Ä—É–≥–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –ø–æ–¥–ø–∏—Å–∏
            '—É–≤–∞–∂–µ–Ω–∏–µ–º', '–±–ª–∞–≥–æ–¥–∞—Ä–Ω–æ—Å—Ç—å—é', '–Ω–∞–¥–µ–∂–¥–æ–π'
        ]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –ª–∏ –æ—Ç–≤–µ—Ç –æ–¥–Ω–∏–º –∏–∑ –¥–æ–ø—É—Å—Ç–∏–º—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤
        for ending in valid_endings:
            if response.endswith(ending):
                return True
                
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - –µ—Å–ª–∏ –ø–∏—Å—å–º–æ —Å–æ–¥–µ—Ä–∂–∏—Ç "–° —É–≤–∞–∂–µ–Ω–∏–µ–º" –≤ –∫–æ–Ω—Ü–µ
        if '—É–≤–∞–∂–µ–Ω–∏–µ–º' in response[-100:] or '–±–ª–∞–≥–æ–¥–∞—Ä–Ω–æ—Å—Ç—å—é' in response[-100:]:
            return True
            
        logger.warning(f"–û—Ç–≤–µ—Ç –Ω–µ –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ: '{response[-50:]}'")
        return False 

    async def get_completion(
        self, 
        prompt: str, 
        temperature: float = 0.7, 
        max_tokens: int = 1500
    ) -> Optional[str]:
        """
        –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç GPT
        
        Args:
            prompt: –ü—Ä–æ–º–ø—Ç –¥–ª—è GPT
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            
        Returns:
            –û—Ç–≤–µ—Ç –æ—Ç GPT –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        """
        try:
            logger.info(f"ü§ñ –û—Ç–ø—Ä–∞–≤–ª—è—é –∑–∞–ø—Ä–æ—Å –∫ GPT (temp={temperature}, max_tokens={max_tokens}, timeout={OPENAI_TIMEOUT}s)")
            logger.info(f"üìù –î–ª–∏–Ω–∞ –ø—Ä–æ–º–ø—Ç–∞: {len(prompt)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            response = await asyncio.wait_for(
                self.client.chat.completions.create(
                    model=OPENAI_MODEL,
                    messages=[
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    max_tokens=max_tokens,
                    temperature=temperature
                ),
                timeout=OPENAI_TIMEOUT
            )
            
            if response.choices and response.choices[0].message.content:
                content = response.choices[0].message.content
                logger.info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –æ—Ç GPT: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")
                return content
            else:
                logger.error("‚ùå GPT –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç")
                return None
                
        except asyncio.TimeoutError:
            logger.error("‚ùå –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ –∫ GPT")
            return None
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ GPT: {e}")
            
            # –ü—Ä–æ–±—É–µ–º fallback –º–æ–¥–µ–ª—å
            try:
                logger.info(f"üîÑ –ü—Ä–æ–±—É—é fallback –º–æ–¥–µ–ª—å {OPENAI_FALLBACK_MODEL}...")
                response = await asyncio.wait_for(
                    self.client.chat.completions.create(
                        model=OPENAI_FALLBACK_MODEL,
                        messages=[
                            {
                                "role": "user",
                                "content": prompt
                            }
                        ],
                        max_tokens=max_tokens,
                        temperature=temperature
                    ),
                    timeout=OPENAI_TIMEOUT
                )
                
                if response.choices and response.choices[0].message.content:
                    content = response.choices[0].message.content
                    logger.info(f"‚úÖ Fallback –º–æ–¥–µ–ª—å –æ—Ç–≤–µ—Ç–∏–ª–∞: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")
                    return content
                else:
                    logger.error("‚ùå Fallback –º–æ–¥–µ–ª—å –≤–µ—Ä–Ω—É–ª–∞ –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç")
                    return None
                    
            except Exception as fallback_e:
                logger.error(f"‚ùå Fallback –º–æ–¥–µ–ª—å —Ç–æ–∂–µ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç: {fallback_e}")
                return None

    async def generate_personalized_letter(self, prompt: str, temperature: Optional[float] = None) -> Optional[str]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–∏—Å—å–º–æ –ø–æ –≥–æ—Ç–æ–≤–æ–º—É –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É –ø—Ä–æ–º–ø—Ç—É
        
        Args:
            prompt: –ì–æ—Ç–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å)
            
        Returns:
            –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø–∏—Å—å–º–æ –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        """
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—É—é —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É –∏–ª–∏ –±–µ—Ä–µ–º –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        temp = temperature if temperature is not None else OPENAI_TEMPERATURE
        
        # –ü—Ä–æ–±—É–µ–º —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–∏—Å—å–º–æ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏
        for attempt in range(MAX_GENERATION_ATTEMPTS):
            try:
                logger.info(f"–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è #{attempt + 1} (temp={temp})")
                
                response = await asyncio.wait_for(
                    self._make_personalized_request(prompt, temp),
                    timeout=OPENAI_TIMEOUT
                )
                
                if response and self._is_response_complete(response):
                    logger.info("–£—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø–∏—Å—å–º–æ")
                    return response
                else:
                    logger.warning(f"–ù–µ–ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ #{attempt + 1}")
                    
            except asyncio.TimeoutError:
                logger.error(f"–¢–∞–π–º–∞—É—Ç –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ #{attempt + 1}")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ #{attempt + 1}: {e}")
            
        logger.error("–í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–µ—É—Å–ø–µ—à–Ω—ã")
        return None

    async def _make_personalized_request(self, prompt: str, temperature: float) -> Optional[str]:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å –∫ OpenAI API
        
        Args:
            prompt: –ü—Ä–æ–º–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            
        Returns:
            –û—Ç–≤–µ—Ç –æ—Ç OpenAI –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        """
        try:
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –æ—Å–Ω–æ–≤–Ω—É—é –º–æ–¥–µ–ª—å
            response = await self.client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=[
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                max_tokens=1500,
                temperature=temperature,
                top_p=OPENAI_TOP_P,
                presence_penalty=OPENAI_PRESENCE_PENALTY,
                frequency_penalty=OPENAI_FREQUENCY_PENALTY
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ —Å –º–æ–¥–µ–ª—å—é {OPENAI_MODEL}: {e}")
            
            # –ü—Ä–æ–±—É–µ–º fallback –º–æ–¥–µ–ª—å
            try:
                response = await self.client.chat.completions.create(
                    model=OPENAI_FALLBACK_MODEL,
                    messages=[
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    max_tokens=1500,
                    temperature=temperature,
                    top_p=OPENAI_TOP_P,
                    presence_penalty=OPENAI_PRESENCE_PENALTY,
                    frequency_penalty=OPENAI_FREQUENCY_PENALTY
                )
                
                return response.choices[0].message.content
                
            except Exception as fallback_e:
                logger.error(f"–û—à–∏–±–∫–∞ —Å fallback –º–æ–¥–µ–ª—å—é {OPENAI_FALLBACK_MODEL}: {fallback_e}")
                return None


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–≤–∏—Å–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞  
openai_service = OpenAIService()


async def generate_letter_with_retry(prompt: str, temperature: Optional[float] = None) -> Optional[str]:
    """
    –£–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–∏—Å—å–º–∞ —Å –≥–æ—Ç–æ–≤—ã–º –ø—Ä–æ–º–ø—Ç–æ–º
    """
    return await openai_service.generate_personalized_letter(prompt, temperature) 