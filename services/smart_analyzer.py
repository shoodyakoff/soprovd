"""
–£–º–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä v6.0 - –µ–¥–∏–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ GPT
–†–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: 1 –∑–∞–ø—Ä–æ—Å –≤–º–µ—Å—Ç–æ 4, –±—ã—Å—Ç—Ä–µ–µ –≤ 2-3 —Ä–∞–∑–∞

üö®üö®üö® DEPRECATED WARNING üö®üö®üö®
–≠—Ç–æ—Ç —Ñ–∞–π–ª –£–°–¢–ê–†–ï–õ –∏ —Å–æ–¥–µ—Ä–∂–∏—Ç legacy –∫–æ–¥!

–ò–°–ü–û–õ–¨–ó–£–ô–¢–ï –í–ú–ï–°–¢–û –ù–ï–ì–û: services/smart_analyzer_v6.py

–ü–†–ò–ß–ò–ù–´ –ó–ê–ú–ï–ù–´:
- –≠—Ç–æ—Ç —Ñ–∞–π–ª: 956 —Å—Ç—Ä–æ–∫ legacy –∫–æ–¥–∞ —Å backward compatibility
- v6.py —Ñ–∞–π–ª: 400+ —Å—Ç—Ä–æ–∫ —Ç–æ–ª—å–∫–æ –∞–∫—Ç—É–∞–ª—å–Ω–æ–π –ª–æ–≥–∏–∫–∏
- –î—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Å–æ–∑–¥–∞–µ—Ç –ø—É—Ç–∞–Ω–∏—Ü—É
- v6.py –∏–º–µ–µ—Ç –≤—Å—Ç—Ä–æ–µ–Ω–Ω—É—é –¥–µ-–ò–ò-—Ñ–∏–∫–∞—Ü–∏—é –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
- v6.py –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

–ü–õ–ê–ù –ú–ò–ì–†–ê–¶–ò–ò:
1. –ó–∞–º–µ–Ω–∏—Ç–µ –∏–º–ø–æ—Ä—Ç—ã: 
   from services.smart_analyzer import ... ‚Üí from services.smart_analyzer_v6 import ...
2. –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ —Ä–∞–±–æ—Ç—É
3. –£–¥–∞–ª–∏—Ç–µ —ç—Ç–æ—Ç —Ñ–∞–π–ª –ø–æ—Å–ª–µ –ø–æ–ª–Ω–æ–π –º–∏–≥—Ä–∞—Ü–∏–∏

üö®üö®üö® DEPRECATED WARNING üö®üö®üö®
"""
import json
import logging
import os
import re
from typing import Dict, Any, Optional, Tuple

logger = logging.getLogger(__name__)


# ============ –ü–†–û–ú–ü–¢–´ v6.0 ============

UNIFIED_ANALYSIS_PROMPT = """
–ó–ê–î–ê–ß–ê: –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –≤–∞–∫–∞–Ω—Å–∏—é –∏ —Ä–µ–∑—é–º–µ –û–î–ù–û–í–†–ï–ú–ï–ù–ù–û, –Ω–∞–π–¥–∏ —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –∏ –Ω–∞–ø–∏—à–∏ —Å–æ–ø—Ä–æ–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ–µ –ø–∏—Å—å–º–æ.

–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û:
1. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –û–ë–ê —Ç–µ–∫—Å—Ç–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ, –Ω–µ –ø–æ –æ—Ç–¥–µ–ª—å–Ω–æ—Å—Ç–∏
2. –ò—â–∏ –ö–û–ù–ö–†–ï–¢–ù–´–ï —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è, –∞ –Ω–µ –æ–±—â–∏–µ —Ç–µ–º—ã
3. –ö–∞–∂–¥–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ–¥–∫—Ä–µ–ø–ª—è–π –¢–û–ß–ù–û–ô –¶–ò–¢–ê–¢–û–ô –∏–∑ —Ä–µ–∑—é–º–µ
4. –ù–ï –°–ú–ï–®–ò–í–ê–ô —Ä–∞–∑–Ω—ã–µ –∫–æ–º–ø–∞–Ω–∏–∏ –∏ –ø—Ä–æ–µ–∫—Ç—ã –≤ –æ–¥–Ω—É –∏—Å—Ç–æ—Ä–∏—é
5. –ü–∏—Å—å–º–æ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –°–í–Ø–ó–ù–û–ô –ò–°–¢–û–†–ò–ï–ô, –∞ –Ω–µ —Å–ø–∏—Å–∫–æ–º —Ñ–∞–∫—Ç–æ–≤

–ê–õ–ì–û–†–ò–¢–ú –ü–û–ò–°–ö–ê –°–û–í–ü–ê–î–ï–ù–ò–ô:
1. –ü–†–û–ß–ò–¢–ê–ô –≤–∞–∫–∞–Ω—Å–∏—é –∏ –≤—ã–¥–µ–ª–∏ –í–°–ï –∫–ª—é—á–µ–≤—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è (–≤–∫–ª—é—á–∞—è –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏, –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã, –ø—Ä–æ—Ü–µ—Å—Å—ã)
2. –î–ª—è –ö–ê–ñ–î–û–ì–û —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∏—â–∏ –≤ —Ä–µ–∑—é–º–µ:
   - –ü–†–Ø–ú–´–ï —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è (–æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã: "OKR" = "OKR")
   - –°–ú–ï–ñ–ù–´–ï —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è (–ø–æ—Ö–æ–∂–∏–µ –ø—Ä–æ—Ü–µ—Å—Å—ã)
   - –û–¢–°–£–¢–°–¢–í–ò–ï (–µ—Å–ª–∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –Ω–µ—Ç –≤ —Ä–µ–∑—é–º–µ)
3. –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –Ω–∞–π–¥–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é —Ü–∏—Ç–∞—Ç—É-–¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ
4. –û—Ü–µ–Ω–∏ —Å–∏–ª—É –∫–∞–∂–¥–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è (0.0-1.0)

–ü–†–ê–í–ò–õ–ê –ö–û–ù–¢–ï–ö–°–¢–ê:
- –ù–ï –æ–±—ä–µ–¥–∏–Ω—è–π –æ–ø—ã—Ç –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∫–æ–º–ø–∞–Ω–∏–π –≤ –æ–¥–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
- –ß–µ—Ç–∫–æ —Ä–∞–∑–¥–µ–ª—è–π –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –ø–æ –º–µ—Å—Ç–∞–º —Ä–∞–±–æ—Ç—ã
- –ï—Å–ª–∏ —É–ø–æ–º–∏–Ω–∞–µ—à—å —Ü–∏—Ñ—Ä—ã - —É–∫–∞–∑—ã–≤–∞–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (–∫–∞–∫–∞—è –∫–æ–º–ø–∞–Ω–∏—è)
- –ü—Ä–∏–º–µ—Ä: "–í PRYSM –∑–∞–ø—É—Å—Ç–∏–ª MVP –∑–∞ 4 –º–µ—Å—è—Ü–∞. –í –¢-–ë–∞–Ω–∫–µ —Å–æ—Ö—Ä–∞–Ω–∏–ª –ø—Ä–æ–¥–∞–∂–∏ –Ω–∞ XXX –º–ª–Ω NPV"

–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–´–ï –¢–†–ï–ë–û–í–ê–ù–ò–Ø –î–õ–Ø –ü–û–ò–°–ö–ê:
- OKR (–µ—Å–ª–∏ –µ—Å—Ç—å –≤ –≤–∞–∫–∞–Ω—Å–∏–∏)
- A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ (–µ—Å–ª–∏ –µ—Å—Ç—å –≤ –≤–∞–∫–∞–Ω—Å–∏–∏)  
- Scrum/Agile (–µ—Å–ª–∏ –µ—Å—Ç—å –≤ –≤–∞–∫–∞–Ω—Å–∏–∏)
- –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã
- –û—Ç—Ä–∞—Å–ª–µ–≤–æ–π –æ–ø—ã—Ç (TravelTech, HotelTech, PropTech, FinTech)
- –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥–∞–º–∏ (—Ä–∞–∑–º–µ—Ä –∫–æ–º–∞–Ω–¥—ã)
- –ó–∞–ø—É—Å–∫ –ø—Ä–æ–¥—É–∫—Ç–æ–≤
- –†–∞–±–æ—Ç–∞ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏

–ü–†–ò–ù–¶–ò–ü–´ –°–í–Ø–ó–ù–û–ô –ò–°–¢–û–†–ò–ò:
- –ö–∞–∂–¥–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –ª–æ–≥–∏—á–µ—Å–∫–∏ –≤—ã—Ç–µ–∫–∞–µ—Ç –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ
- –ò—Å–ø–æ–ª—å–∑—É–π —Å–≤—è–∑–∫–∏: "–±–ª–∞–≥–æ–¥–∞—Ä—è —ç—Ç–æ–º—É", "–∏–º–µ–Ω–Ω–æ –ø–æ—ç—Ç–æ–º—É", "—ç—Ç–æ –ø–æ–∑–≤–æ–ª–∏–ª–æ"
- –ü–æ–∫–∞–∂–∏ –≠–í–û–õ–Æ–¶–ò–Æ –Ω–∞–≤—ã–∫–æ–≤: –æ—Ç –ø—Ä–æ—Å—Ç–æ–≥–æ –∫ —Å–ª–æ–∂–Ω–æ–º—É
- –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ü–∏—Ñ—Ä—ã –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –° –£–ö–ê–ó–ê–ù–ò–ï–ú –ö–û–ù–¢–ï–ö–°–¢–ê

–°–¢–†–£–ö–¢–£–†–ê –ü–ò–°–¨–ú–ê (–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û 4 –ê–ë–ó–ê–¶–ê –° –ü–ï–†–ï–ù–û–°–ê–ú–ò):
–ê–±–∑–∞—Ü 1 (30-40 —Å–ª–æ–≤): –ó–∞—Ü–µ–ø–∫–∞ + —Å–∞–º–æ–µ —Å–∏–ª—å–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (–û–î–ù–ê –∫–æ–º–ø–∞–Ω–∏—è)

–ê–±–∑–∞—Ü 2 (50-60 —Å–ª–æ–≤): –†–∞–∑–≤–∏—Ç–∏–µ –Ω–∞–≤—ã–∫–æ–≤ —á–µ—Ä–µ–∑ –ª–æ–≥–∏—á–µ—Å–∫—É—é —Ü–µ–ø–æ—á–∫—É (–º–æ–∂–Ω–æ –¥—Ä—É–≥–∞—è –∫–æ–º–ø–∞–Ω–∏—è)

–ê–±–∑–∞—Ü 3 (40-50 —Å–ª–æ–≤): –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–ø—ã—Ç–∞ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

–ê–±–∑–∞—Ü 4 (20-30 —Å–ª–æ–≤): –ü—Ä–∏–∑—ã–≤ –∫ –¥–µ–π—Å—Ç–≤–∏—é

–ë–õ–û–ö –í–û–ü–†–û–°–û–í –î–õ–Ø –£–õ–£–ß–®–ï–ù–ò–Ø:
–ï—Å–ª–∏ –≤ –≤–∞–∫–∞–Ω—Å–∏–∏ –µ—Å—Ç—å —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã—Ö –ù–ï–¢ –≤ —Ä–µ–∑—é–º–µ, –ø—Ä–µ–¥–ª–æ–∂–∏ 3-5 —É—Ç–æ—á–Ω—è—é—â–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤:
- "–ï—Å—Ç—å –ª–∏ —É –≤–∞—Å –æ–ø—ã—Ç —Å A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º?"
- "–†–∞–±–æ—Ç–∞–ª–∏ –ª–∏ –≤—ã —Å OKR –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–µ–π?"
- "–ö–∞–∫–æ–π —É –≤–∞—Å –æ–ø—ã—Ç –≤ TravelTech –∏–Ω–¥—É—Å—Ç—Ä–∏–∏?"

–í–ê–ö–ê–ù–°–ò–Ø:
{vacancy_text}

–†–ï–ó–Æ–ú–ï:
{resume_text}

–°–¢–ò–õ–¨ –ü–ò–°–¨–ú–ê: {style}

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê: —Å—Ç—Ä–æ–≥–æ JSON –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
{{
  "matches": [
    {{
      "requirement": "–∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ –∏–∑ –≤–∞–∫–∞–Ω—Å–∏–∏",
      "evidence": "–Ω–∞–π–¥–µ–Ω–Ω—ã–π –æ–ø—ã—Ç –≤ —Ä–µ–∑—é–º–µ",
      "quote": "—Ç–æ—á–Ω–∞—è —Ü–∏—Ç–∞—Ç–∞ –∏–∑ —Ä–µ–∑—é–º–µ",
      "company_context": "–≤ –∫–∞–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–∏ –±—ã–ª–æ",
      "strength": 0.95,
      "explanation": "–ø–æ—á–µ–º—É —ç—Ç–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤–∞–∂–Ω–æ"
    }}
  ],
  "missing_requirements": [
    {{
      "requirement": "—á—Ç–æ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ —Ä–µ–∑—é–º–µ",
      "importance": "high/medium/low",
      "suggested_question": "–≤–æ–ø—Ä–æ—Å –¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–∏—è"
    }}
  ],
  "clarifying_questions": [
    "–í–æ–ø—Ä–æ—Å 1 –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–µ–∑—é–º–µ",
    "–í–æ–ø—Ä–æ—Å 2 –¥–ª—è —Å–±–æ—Ä–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞",
    "–í–æ–ø—Ä–æ—Å 3 –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è —Å–∫—Ä—ã—Ç–æ–≥–æ –æ–ø—ã—Ç–∞"
  ],
  "letter": "–ê–ë–ó–ê–¶ 1: —Ç–µ–∫—Å—Ç –ø–µ—Ä–≤–æ–≥–æ –∞–±–∑–∞—Ü–∞\\n\\n–ê–ë–ó–ê–¶ 2: —Ç–µ–∫—Å—Ç –≤—Ç–æ—Ä–æ–≥–æ –∞–±–∑–∞—Ü–∞\\n\\n–ê–ë–ó–ê–¶ 3: —Ç–µ–∫—Å—Ç —Ç—Ä–µ—Ç—å–µ–≥–æ –∞–±–∑–∞—Ü–∞\\n\\n–ê–ë–ó–ê–¶ 4: —Ç–µ–∫—Å—Ç —á–µ—Ç–≤–µ—Ä—Ç–æ–≥–æ –∞–±–∑–∞—Ü–∞",
  "confidence": 0.85
}}
"""

CONTEXTUAL_MATCHING_PROMPT = """
–ó–ê–î–ê–ß–ê: –ù–∞–π–¥–∏ –¢–û–ß–ù–´–ï —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –º–µ–∂–¥—É –≤–∞–∫–∞–Ω—Å–∏–µ–π –∏ —Ä–µ–∑—é–º–µ.

–ü–†–ê–í–ò–õ–ê:
1. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –û–ë–ê —Ç–µ–∫—Å—Ç–∞ –û–î–ù–û–í–†–ï–ú–ï–ù–ù–û
2. –ò—â–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è, –Ω–µ –∞–±—Å—Ç—Ä–∞–∫—Ü–∏–∏
3. –ö–∞–∂–¥–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ = —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ + –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ + —Ü–∏—Ç–∞—Ç–∞
4. –ò–≥–Ω–æ—Ä–∏—Ä—É–π –æ–±—â–∏–µ –Ω–∞–≤—ã–∫–∏ (–∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—è, –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å)
5. –§–æ–∫—É—Å –Ω–∞ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è—Ö, –ø—Ä–æ—Ü–µ—Å—Å–∞—Ö, —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö

–ü–†–ò–ú–ï–†–´ –•–û–†–û–®–ò–• –°–û–í–ü–ê–î–ï–ù–ò–ô:
- "OKR –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ" ‚Üí "–≤–Ω–µ–¥—Ä–∏–ª OKR –≤ –∫–æ–º–∞–Ω–¥–µ 12 —á–µ–ª–æ–≤–µ–∫" (—Ü–∏—Ç–∞—Ç–∞)
- "Python —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞" ‚Üí "3 –≥–æ–¥–∞ –æ–ø—ã—Ç–∞ —Å Python, Django" (—Ü–∏—Ç–∞—Ç–∞)
- "A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ" ‚Üí "–ø—Ä–æ–≤–µ–ª 15+ A/B —Ç–µ—Å—Ç–æ–≤, —Ä–æ—Å—Ç –∫–æ–Ω–≤–µ—Ä—Å–∏–∏ –Ω–∞ 25%" (—Ü–∏—Ç–∞—Ç–∞)

–ü–†–ò–ú–ï–†–´ –ü–õ–û–•–ò–• –°–û–í–ü–ê–î–ï–ù–ò–ô:
- "–∫–æ–º–º—É–Ω–∏–∫–∞—Ç–∏–≤–Ω—ã–µ –Ω–∞–≤—ã–∫–∏" ‚Üí "—Ä–∞–±–æ—Ç–∞–ª –≤ –∫–æ–º–∞–Ω–¥–µ"
- "–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å" ‚Üí "–≤—ã–ø–æ–ª–Ω—è–ª –∑–∞–¥–∞—á–∏"

–í–ê–ö–ê–ù–°–ò–Ø:
{vacancy_text}

–†–ï–ó–Æ–ú–ï:
{resume_text}

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê: —Å—Ç—Ä–æ–≥–æ JSON
{{
  "matches": [
    {{
      "requirement": "—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ –∏–∑ –≤–∞–∫–∞–Ω—Å–∏–∏",
      "evidence": "–æ–ø—ã—Ç –∏–∑ —Ä–µ–∑—é–º–µ",
      "quote": "—Ç–æ—á–Ω–∞—è —Ü–∏—Ç–∞—Ç–∞",
      "strength": 0.9,
      "type": "exact|strong|weak"
    }}
  ],
  "gaps": [
    {{
      "missing_requirement": "—á–µ–≥–æ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç",
      "potential_bridge": "—á–µ–º –º–æ–∂–Ω–æ –∫–æ–º–ø–µ–Ω—Å–∏—Ä–æ–≤–∞—Ç—å"
    }}
  ]
}}
"""

LETTER_FROM_CONTEXT_PROMPT = """
–ù–∞–ø–∏—à–∏ —Å–æ–ø—Ä–æ–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ–µ –ø–∏—Å—å–º–æ –∫–∞–∫ –°–í–Ø–ó–ù–£–Æ –ò–°–¢–û–†–ò–Æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π.

–ü–†–ò–ù–¶–ò–ü–´:
1. –õ–æ–≥–∏—á–µ—Å–∫–∞—è —Ü–µ–ø–æ—á–∫–∞: –æ–ø—ã—Ç ‚Üí –Ω–∞–≤—ã–∫ ‚Üí —Ä–µ–∑—É–ª—å—Ç–∞—Ç
2. –ü—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–≤—è–∑–∏
3. –ï–¥–∏–Ω–∞—è –Ω–∞—Ä—Ä–∞—Ç–∏–≤–∞
4. –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ü–∏—Ñ—Ä—ã –∏ —Ñ–∞–∫—Ç—ã

–°–í–Ø–ó–£–Æ–©–ò–ï –ö–û–ù–°–¢–†–£–ö–¶–ò–ò:
- "–≠—Ç–æ –Ω–∞—É—á–∏–ª–æ –º–µ–Ω—è..."
- "–ë–ª–∞–≥–æ–¥–∞—Ä—è –æ–ø—ã—Ç—É –≤ X, —è —Å–º–æ–≥..."
- "–ò–º–µ–Ω–Ω–æ –ø–æ—ç—Ç–æ–º—É..."
- "–†–∞–∑–≤–∏–≤–∞—è –Ω–∞–≤—ã–∫–∏ –≤ X, —è –ø–µ—Ä–µ—à–µ–ª –∫ Y..."
- "–ö–æ–º–±–∏–Ω–∏—Ä—É—è X –∏ Y, —è –¥–æ—Å—Ç–∏–≥..."

–°–¢–†–£–ö–¢–£–†–ê:
–ê–±–∑–∞—Ü 1: –ó–∞—Ü–µ–ø–∫–∞ + —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –Ω–∞–≤—ã–∫
–ê–±–∑–∞—Ü 2: –†–∞–∑–≤–∏—Ç–∏–µ —á–µ—Ä–µ–∑ –ª–æ–≥–∏—á–µ—Å–∫—É—é —Ü–µ–ø–æ—á–∫—É
–ê–±–∑–∞—Ü 3: –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
–ê–±–∑–∞—Ü 4: –ü—Ä–∏–∑—ã–≤ –∫ –¥–µ–π—Å—Ç–≤–∏—é

–°–û–í–ü–ê–î–ï–ù–ò–Ø:
{matches}

–ò–°–•–û–î–ù–ê–Ø –í–ê–ö–ê–ù–°–ò–Ø:
{vacancy_text}

–ò–°–•–û–î–ù–û–ï –†–ï–ó–Æ–ú–ï:
{resume_text}

–°–¢–ò–õ–¨: {style}

–ù–∞–ø–∏—à–∏ –¢–û–õ–¨–ö–û —Ç–µ–∫—Å—Ç –ø–∏—Å—å–º–∞ –±–µ–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤.
"""

DEAI_PROMPT = """–ü—Ä–æ–≤–µ—Ä—å —Ç–µ–∫—Å—Ç –Ω–∞ –ò–ò-—à—Ç–∞–º–ø—ã –∏ —Å–¥–µ–ª–∞–π –µ–≥–æ –±–æ–ª–µ–µ —á–µ–ª–æ–≤–µ—á–Ω—ã–º.

–î–ï–¢–ï–ö–¢–ò–†–£–ï–ú–´–ï –ü–ê–¢–¢–ï–†–ù–´:
- "—Ö–æ—Ç–µ–ª –±—ã –≤—ã—Ä–∞–∑–∏—Ç—å –∑–∞–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å" 
- "—Ä–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –º–æ—é –∫–∞–Ω–¥–∏–¥–∞—Ç—É—Ä—É"
- "–∏–¥–µ–∞–ª—å–Ω–æ –ø–æ–¥—Ö–æ–∂—É –¥–ª—è –ø–æ–∑–∏—Ü–∏–∏"
- "—É–Ω–∏–∫–∞–ª—å–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è –Ω–∞–≤—ã–∫–æ–≤"
- "–≥–æ—Ç–æ–≤ –≤–Ω–µ—Å—Ç–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π –≤–∫–ª–∞–¥"
- "–±—É–¥—É —Ä–∞–¥ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –æ–±—Å—É–¥–∏—Ç—å"
- "—Å –Ω–µ—Ç–µ—Ä–ø–µ–Ω–∏–µ–º –∂–¥—É –≤–∞—à–µ–≥–æ –æ—Ç–≤–µ—Ç–∞"
- "—Å—Ç—Ä–∞—Å—Ç–Ω–æ —É–≤–ª–µ—á–µ–Ω"
- "—Ç–∞–∫–∂–µ —Å—Ç–æ–∏—Ç —É–ø–æ–º—è–Ω—É—Ç—å"
- "–∫—Ä–æ–º–µ —Ç–æ–≥–æ, —Ö–æ—á—É –æ—Ç–º–µ—Ç–∏—Ç—å"

–ó–ê–ú–ï–ù–ò –ù–ê –ß–ï–õ–û–í–ï–ß–ù–´–ï –í–ê–†–ò–ê–ù–¢–´:
- "–∑–∞–º–µ—Ç–∏–ª –≤–∞—à—É –≤–∞–∫–∞–Ω—Å–∏—é –∏ –ø–æ–Ω—è–ª - —ç—Ç–æ —Ç–æ, —á—Ç–æ –∏—Å–∫–∞–ª"
- "–≤–∞—à–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ –ø—Ä–∏–≤–ª–µ–∫–ª–æ –≤–Ω–∏–º–∞–Ω–∏–µ"
- "–¥—É–º–∞—é, –º–æ–π –æ–ø—ã—Ç –∑–¥–µ—Å—å –ø—Ä–∏–≥–æ–¥–∏—Ç—Å—è"
- "–≤ [–∫–æ–º–ø–∞–Ω–∏—è] –º–Ω–µ –¥–æ–≤–µ–ª–æ—Å—å"
- "–∫–æ–≥–¥–∞ —Ä–∞–±–æ—Ç–∞–ª –Ω–∞–¥ [–ø—Ä–æ–µ–∫—Ç], —Å—Ç–æ–ª–∫–Ω—É–ª—Å—è —Å"
- "–∑–∞ X –ª–µ—Ç —Ä–∞–±–æ—Ç—ã –≤ [—Å—Ñ–µ—Ä–∞] –Ω–∞—É—á–∏–ª—Å—è"

–ü–†–ê–í–ò–õ–ê –£–õ–£–ß–®–ï–ù–ò–Ø:
1. –£–±–µ—Ä–∏ –≤—Å–µ —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–µ –æ–±–æ—Ä–æ—Ç—ã
2. –î–æ–±–∞–≤—å –∂–∏–≤—ã–µ –¥–µ—Ç–∞–ª–∏ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç
3. –ó–∞–º–µ–Ω–∏ –æ–±—â–∏–µ —Ñ—Ä–∞–∑—ã –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã
4. –°–¥–µ–ª–∞–π –ø–µ—Ä–µ—Ö–æ–¥—ã –º–µ–∂–¥—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏
5. –î–æ–±–∞–≤—å —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—É—é —Å–æ—Å—Ç–∞–≤–ª—è—é—â—É—é, –Ω–æ –±–µ–∑ –ø–∞—Ñ–æ—Å–∞

–¢–ï–ö–°–¢ –î–õ–Ø –ü–†–û–í–ï–†–ö–ò:
{text}

–í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ —É–ª—É—á—à–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –∏ –ø–æ–º–µ—Ç–æ–∫."""


# ============ LEGACY –ü–†–û–ú–ü–¢–´ (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏) ============
# –ê—Ä—Ö–∏–≤–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω—ã –≤ prompts_archive.md

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º legacy –ø—Ä–æ–º–ø—Ç—ã —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
def _get_legacy_prompts():
    """–õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ legacy –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
    # –í —Å–ª—É—á–∞–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –º–æ–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ prompts_archive.md
    # –ü–æ–∫–∞ –æ—Å—Ç–∞–≤–ª—è–µ–º –∑–∞–≥–ª—É—à–∫–∏ –¥–ª—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤
    
    DEEP_ANALYSIS_PROMPT = """–¢—ã —ç–∫—Å–ø–µ—Ä—Ç-–∞–Ω–∞–ª–∏—Ç–∏–∫ –ø–æ HR –∏ —Ä–µ–∫—Ä—É—Ç–∏–Ω–≥—É. –ü—Ä–æ–≤–µ–¥–∏ –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –≤–∞–∫–∞–Ω—Å–∏–∏ –∏ —Ä–µ–∑—é–º–µ.

–ó–ê–î–ê–ß–ê: –ü–æ—à–∞–≥–æ–≤–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∏ –≤–µ—Ä–Ω–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π JSON

–≠–¢–ê–ü–´ –ê–ù–ê–õ–ò–ó–ê:
1. –ê–ù–ê–õ–ò–ó –í–ê–ö–ê–ù–°–ò–ò - –∏—â–∏ –ö–û–ù–ö–†–ï–¢–ù–´–ï —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
2. –ê–ù–ê–õ–ò–ó –†–ï–ó–Æ–ú–ï - –∏—â–∏ –¢–û–ß–ù–´–ï —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è  
3. –ü–û–ò–°–ö –ü–†–Ø–ú–´–• –°–û–í–ü–ê–î–ï–ù–ò–ô

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê: —Å—Ç—Ä–æ–≥–æ JSON –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
{{
  "vacancy_analysis": {{
    "key_requirements": ["—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ1", "—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ2"],
    "pain_points": ["–ø—Ä–æ–±–ª–µ–º–∞1", "–ø—Ä–æ–±–ª–µ–º–∞2"], 
    "priorities": ["–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç1", "–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç2"],
    "company_culture": "–æ–ø–∏—Å–∞–Ω–∏–µ –∫—É–ª—å—Ç—É—Ä—ã",
    "urgency_signals": ["—Å–∏–≥–Ω–∞–ª1", "—Å–∏–≥–Ω–∞–ª2"],
    "hidden_needs": ["—Å–∫—Ä—ã—Ç–∞—è –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç—å1", "—Å–∫—Ä—ã—Ç–∞—è –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç—å2"]
  }},
  "resume_analysis": {{
    "key_skills": ["–Ω–∞–≤—ã–∫1", "–Ω–∞–≤—ã–∫2"],
    "achievements": ["–¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏1", "–¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏2"],
    "unique_advantages": ["–£–¢–ü1", "–£–¢–ü2"],
    "career_trajectory": "–æ–ø–∏—Å–∞–Ω–∏–µ –∫–∞—Ä—å–µ—Ä–Ω–æ–≥–æ –ø—É—Ç–∏",
    "transferable_skills": ["–ø–µ—Ä–µ–Ω–æ—Å–∏–º—ã–π –Ω–∞–≤—ã–∫1", "–ø–µ—Ä–µ–Ω–æ—Å–∏–º—ã–π –Ω–∞–≤—ã–∫2"]
  }},
  "matching_strategy": {{
    "direct_matches": ["—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ1", "—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ2"],
    "skill_gaps": ["–ø—Ä–æ–±–µ–ª1", "–ø—Ä–æ–±–µ–ª2"],
    "positioning": "—Å—Ç—Ä–∞—Ç–µ–≥–∏—è –ø–æ–∑–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è",
    "value_proposition": "—Ü–µ–Ω–Ω–æ—Å—Ç–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ",
    "specific_references": ["–∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è –æ—Ç—Å—ã–ª–∫–∞ –∫ –≤–∞–∫–∞–Ω—Å–∏–∏1", "–∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è –æ—Ç—Å—ã–ª–∫–∞ –∫ –≤–∞–∫–∞–Ω—Å–∏–∏2"]
  }},
  "confidence_score": 0.85
}}

–í–ê–ö–ê–ù–°–ò–Ø:
{vacancy_text}

–†–ï–ó–Æ–ú–ï:
{resume_text}"""

    HUMAN_WRITING_PROMPT = """–¢—ã –ø–∏—à–µ—à—å —Å–æ–ø—Ä–æ–≤–æ–¥–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∏—Å—å–º–∞ –¥–ª—è –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –Ω–∞ —Ä–∞–±–æ—Ç—É.

–ê–ù–ê–õ–ò–ó –ò–ó –ü–†–ï–î–´–î–£–©–ï–ì–û –≠–¢–ê–ü–ê:
{analysis_json}

–°–¢–ò–õ–¨: {writing_style}

–ù–∞–ø–∏—à–∏ –¢–û–õ–¨–ö–û —Ç–µ–∫—Å—Ç –ø–∏—Å—å–º–∞ –±–µ–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤."""

    return {
        'DEEP_ANALYSIS_PROMPT': DEEP_ANALYSIS_PROMPT,
        'HUMAN_WRITING_PROMPT': HUMAN_WRITING_PROMPT
    }


# ============ –û–°–ù–û–í–ù–û–ô –ö–õ–ê–°–° ============

class SmartAnalyzer:
    """–£–º–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä v6.0 –Ω–∞ –æ—Å–Ω–æ–≤–µ –µ–¥–∏–Ω–æ–≥–æ GPT –ø—Ä–æ–º–ø—Ç–∞"""
    
    def __init__(self, ai_service):
        self.ai = ai_service
        self.user_id: Optional[int] = None
        self.session_id: Optional[str] = None
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ AI –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è —Å–µ—Å—Å–∏–∏
        self.total_tokens_used: int = 0
        self.models_used: set = set()
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º callback –¥–ª—è —Å–±–æ—Ä–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        self.ai.set_stats_callback(self._collect_stats)
    
    def _collect_stats(self, model: str, tokens: int):
        """–°–æ–±–∏—Ä–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É AI –∑–∞–ø—Ä–æ—Å–æ–≤"""
        self.total_tokens_used += tokens
        self.models_used.add(model)
    
    def optimize_input_texts(self, vacancy: str, resume: str) -> Tuple[str, str]:
        """
        –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤
        - –£–¥–∞–ª–µ–Ω–∏–µ –∏–∑–±—ã—Ç–æ—á–Ω—ã—Ö –ø—Ä–æ–±–µ–ª–æ–≤
        - –°–∂–∞—Ç–∏–µ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —Ñ—Ä–∞–∑
        - –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–ª—é—á–µ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        """
        def clean_text(text: str) -> str:
            # –£–¥–∞–ª—è–µ–º –∏–∑–±—ã—Ç–æ—á–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
            text = re.sub(r'\s+', ' ', text.strip())
            # –£–¥–∞–ª—è–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Ñ—Ä–∞–∑—ã (–ø—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞)
            text = re.sub(r'(\b\w+\b)(\s+\1){2,}', r'\1', text)
            return text
        
        optimized_vacancy = clean_text(vacancy)
        optimized_resume = clean_text(resume)
        
        # –õ–æ–≥–∏—Ä—É–µ–º —ç–∫–æ–Ω–æ–º–∏—é —Ç–æ–∫–µ–Ω–æ–≤ (–ø—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: 1 —Ç–æ–∫–µ–Ω ‚âà 4 —Å–∏–º–≤–æ–ª–∞)
        original_chars = len(vacancy) + len(resume)
        optimized_chars = len(optimized_vacancy) + len(optimized_resume)
        saved_chars = original_chars - optimized_chars
        
        if saved_chars > 0:
            logger.info(f"üìä –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞: —Å—ç–∫–æ–Ω–æ–º–ª–µ–Ω–æ ~{saved_chars//4} —Ç–æ–∫–µ–Ω–æ–≤")
        
        return optimized_vacancy, optimized_resume
    
    async def analyze_and_generate_unified(
        self,
        vacancy_text: str,
        resume_text: str,
        style: str = "professional"
    ) -> Dict[str, Any]:
        """
        üöÄ –û–°–ù–û–í–ù–û–ô v6.0: –ï–¥–∏–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–∏—Å—å–º–∞ –∑–∞ –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å
        
        Args:
            vacancy_text: –¢–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏
            resume_text: –¢–µ–∫—Å—Ç —Ä–µ–∑—é–º–µ
            style: –°—Ç–∏–ª—å –ø–∏—Å—å–º–∞
            
        Returns:
            {
                'letter': str,           # –ì–æ—Ç–æ–≤–æ–µ –ø–∏—Å—å–º–æ
                'matches': List[Dict],   # –ù–∞–π–¥–µ–Ω–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
                'analysis_summary': str, # –ö—Ä–∞—Ç–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                'stats': Dict           # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤
            }
        """
        logger.info("üöÄ v6.0: –ó–∞–ø—É—Å–∫–∞—é –µ–¥–∏–Ω—ã–π –∞–Ω–∞–ª–∏–∑...")
        logger.info(f"üìä –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: –≤–∞–∫–∞–Ω—Å–∏—è={len(vacancy_text)} —Å–∏–º–≤–æ–ª–æ–≤, —Ä–µ–∑—é–º–µ={len(resume_text)} —Å–∏–º–≤–æ–ª–æ–≤")
        import time
        start_time = time.time()
        
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º –≤—Ö–æ–¥–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
        optimized_vacancy, optimized_resume = self.optimize_input_texts(vacancy_text, resume_text)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –µ–¥–∏–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        prompt = UNIFIED_ANALYSIS_PROMPT.format(
            vacancy_text=optimized_vacancy,
            resume_text=optimized_resume,
            style=style
        )
        
        try:
            response = await self.ai.get_completion(
                prompt=prompt,
                temperature=0.6,  # –ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å—é –∏ –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å—é
                max_tokens=3000,  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ + –ø–∏—Å—å–º–∞
                user_id=self.user_id,
                session_id=self.session_id,
                request_type="unified_analysis"
            )
            
            if not response:
                logger.error("‚ùå –ü–æ–ª—É—á–µ–Ω –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –ø—Ä–∏ –µ–¥–∏–Ω–æ–º –∞–Ω–∞–ª–∏–∑–µ")
                # Fallback –Ω–∞ —Å—Ç–∞—Ä—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º
                logger.warning("üîÑ –ü–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ legacy –∞–ª–≥–æ—Ä–∏—Ç–º...")
                return await self._legacy_generate_full_analysis_and_letter(vacancy_text, resume_text, style)
            
            # –ü–∞—Ä—Å–∏–º JSON –æ—Ç–≤–µ—Ç
            result = self._parse_json_safely(response, "unified_analysis")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
            if not result.get('letter') or not result.get('matches'):
                logger.error("‚ùå –ù–µ–ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç –æ—Ç –µ–¥–∏–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞")
                # Fallback –Ω–∞ —Å—Ç–∞—Ä—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º
                return await self._legacy_generate_full_analysis_and_letter(vacancy_text, resume_text, style)
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –¥–µ-–ò–ò-—Ñ–∏–∫–∞—Ü–∏—é –∫ –ø–∏—Å—å–º—É
            logger.info("üîß –ü—Ä–∏–º–µ–Ω—è—é –¥–µ-–ò–ò-—Ñ–∏–∫–∞—Ü–∏—é...")
            final_letter = await self.deai_text(result['letter'])
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∞–Ω–∞–ª–∏–∑ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            analysis_summary = self.format_unified_analysis(result.get('matches', []))
            
            execution_time = time.time() - start_time
            logger.info(f"‚úÖ –ï–¥–∏–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {execution_time:.2f}—Å")
            
            return {
                'letter': final_letter,
                'matches': result.get('matches', []),
                'analysis_summary': analysis_summary,
                'missing_requirements': result.get('missing_requirements', []),
                'clarifying_questions': result.get('clarifying_questions', []),
                'stats': {
                    'total_tokens_used': self.total_tokens_used,
                    'models_used': list(self.models_used),
                    'generated_letter_length': len(final_letter),
                    'matches_found': len(result.get('matches', [])),
                    'confidence': result.get('confidence', 0.0),
                    'execution_time': execution_time,
                    'algorithm_version': 'v6.0_unified'
                }
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –µ–¥–∏–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {e}")
            
            # üìä –ê–ù–ê–õ–ò–¢–ò–ö–ê: –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
            try:
                import traceback
                from models.analytics_models import ErrorData
                from services.analytics_service import AnalyticsService
                
                error_data = ErrorData(
                    error_type=type(e).__name__,
                    error_message=str(e),
                    user_id=self.user_id,
                    session_id=self.session_id,
                    stack_trace=traceback.format_exc(),
                    handler_name='analyze_and_generate_unified'
                )
                analytics = AnalyticsService()
                await analytics.log_error(error_data)
            except Exception as log_error:
                logger.error(f"Failed to log unified analysis error: {log_error}")
            
            # Fallback –Ω–∞ —Å—Ç–∞—Ä—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º
            logger.warning("üîÑ –ü–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ legacy –∞–ª–≥–æ—Ä–∏—Ç–º –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏...")
            return await self._legacy_generate_full_analysis_and_letter(vacancy_text, resume_text, style)
    
    def format_unified_analysis(self, matches: list) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –µ–¥–∏–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        """
        try:
            summary = "üìä –ê–ù–ê–õ–ò–ó –°–û–í–ü–ê–î–ï–ù–ò–ô (v6.0)\n\n"
            
            if not matches:
                return summary + "‚ö†Ô∏è –°–æ–≤–ø–∞–¥–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å–∏–ª–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
            sorted_matches = sorted(matches, key=lambda x: x.get('strength', 0), reverse=True)
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-5 —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
            summary += "‚úÖ –ù–ê–ô–î–ï–ù–ù–´–ï –°–û–í–ü–ê–î–ï–ù–ò–Ø:\n"
            for i, match in enumerate(sorted_matches[:5], 1):
                requirement = match.get('requirement', '–¢—Ä–µ–±–æ–≤–∞–Ω–∏–µ')
                evidence = match.get('evidence', '–û–ø—ã—Ç')
                strength = match.get('strength', 0)
                strength_emoji = "üî•" if strength >= 0.9 else "‚ö°" if strength >= 0.7 else "‚úì"
                
                summary += f"{i}. {strength_emoji} {requirement}\n"
                summary += f"   ‚Üí {evidence}\n"
                if match.get('quote'):
                    summary += f"   üí¨ \"{match['quote'][:100]}...\"\n"
                summary += "\n"
            
            # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            avg_strength = sum(m.get('strength', 0) for m in matches) / len(matches)
            summary += f"üìà –°–¢–ê–¢–ò–°–¢–ò–ö–ê:\n"
            summary += f"‚Ä¢ –ù–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {len(matches)}\n"
            summary += f"‚Ä¢ –°—Ä–µ–¥–Ω—è—è —Å–∏–ª–∞: {avg_strength:.2f}\n"
            
            return summary
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞: {e}")
            return "üìä –ê–Ω–∞–ª–∏–∑ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –≤—ã–ø–æ–ª–Ω–µ–Ω, –Ω–æ –≤–æ–∑–Ω–∏–∫–ª–∞ –æ—à–∏–±–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è."
    
    async def analyze_and_generate_contextual(
        self,
        vacancy_text: str,
        resume_text: str,
        style: str = "professional"
    ) -> Dict[str, Any]:
        """
        üîÑ –ê–õ–¨–¢–ï–†–ù–ê–¢–ò–í–ù–´–ô v6.0: –î–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        
        Args:
            vacancy_text: –¢–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏
            resume_text: –¢–µ–∫—Å—Ç —Ä–µ–∑—é–º–µ
            style: –°—Ç–∏–ª—å –ø–∏—Å—å–º–∞
            
        Returns:
            Dict —Å –ø–∏—Å—å–º–æ–º –∏ –∞–Ω–∞–ª–∏–∑–æ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
        """
        logger.info("üîÑ –ê–õ–¨–¢–ï–†–ù–ê–¢–ò–í–ù–´–ô v6.0: –ó–∞–ø—É—Å–∫–∞—é –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º...")
        import time
        start_time = time.time()
        
        try:
            # –≠–¢–ê–ü 1: –ê–Ω–∞–ª–∏–∑ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
            logger.info("üîç –≠–¢–ê–ü 1: –ê–Ω–∞–ª–∏–∑ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π...")
            matches_result = await self._contextual_matching_analysis(vacancy_text, resume_text)
            
            # –≠–¢–ê–ü 2: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–∏—Å—å–º–∞ —Å –ø–æ–ª–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
            logger.info("‚úçÔ∏è –≠–¢–ê–ü 2: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–∏—Å—å–º–∞...")
            letter = await self._generate_letter_from_context(
                matches_result, vacancy_text, resume_text, style
            )
            
            # –î–µ-–ò–ò-—Ñ–∏–∫–∞—Ü–∏—è
            logger.info("üîß –ü—Ä–∏–º–µ–Ω—è—é –¥–µ-–ò–ò-—Ñ–∏–∫–∞—Ü–∏—é...")
            final_letter = await self.deai_text(letter)
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            analysis_summary = self.format_unified_analysis(matches_result.get('matches', []))
            
            execution_time = time.time() - start_time
            logger.info(f"‚úÖ –î–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {execution_time:.2f}—Å")
            
            return {
                'letter': final_letter,
                'matches': matches_result.get('matches', []),
                'analysis_summary': analysis_summary,
                'stats': {
                    'total_tokens_used': self.total_tokens_used,
                    'models_used': list(self.models_used),
                    'generated_letter_length': len(final_letter),
                    'matches_found': len(matches_result.get('matches', [])),
                    'gaps_found': len(matches_result.get('gaps', [])),
                    'execution_time': execution_time,
                    'algorithm_version': 'v6.0_contextual'
                }
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–≤—É—Ö—ç—Ç–∞–ø–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {e}")
            # Fallback –Ω–∞ —Å—Ç–∞—Ä—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º
            return await self._legacy_generate_full_analysis_and_letter(vacancy_text, resume_text, style)
    
    async def _contextual_matching_analysis(self, vacancy_text: str, resume_text: str) -> Dict[str, Any]:
        """–≠—Ç–∞–ø 1: –ê–Ω–∞–ª–∏–∑ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""
        prompt = CONTEXTUAL_MATCHING_PROMPT.format(
            vacancy_text=vacancy_text,
            resume_text=resume_text
        )
        
        response = await self.ai.get_completion(
            prompt=prompt,
            temperature=0.3,
            max_tokens=2000,
            user_id=self.user_id,
            session_id=self.session_id,
            request_type="contextual_matching"
        )
        
        return self._parse_json_safely(response, "contextual_matching")
    
    async def _generate_letter_from_context(
        self, 
        matches_result: Dict[str, Any], 
        vacancy_text: str, 
        resume_text: str, 
        style: str
    ) -> str:
        """–≠—Ç–∞–ø 2: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–∏—Å—å–º–∞ —Å –ø–æ–ª–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""
        prompt = LETTER_FROM_CONTEXT_PROMPT.format(
            matches=json.dumps(matches_result, ensure_ascii=False, indent=2),
            vacancy_text=vacancy_text,
            resume_text=resume_text,
            style=style
        )
        
        response = await self.ai.get_completion(
            prompt=prompt,
            temperature=0.7,
            max_tokens=1500,
            user_id=self.user_id,
            session_id=self.session_id,
            request_type="contextual_letter_generation"
        )
        
        return response.strip() if response else ""
    
    async def deai_text(self, text: str) -> str:
        """
        –î–µ–ò–ò-—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ - —É–±–∏—Ä–∞–µ–º –ò–ò-—à—Ç–∞–º–ø—ã
        
        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            
        Returns:
            –£–ª—É—á—à–µ–Ω–Ω—ã–π —á–µ–ª–æ–≤–µ—á–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        logger.info("üîß –ü—Ä–∏–º–µ–Ω—è—é –¥–µ–ò–ò-—Ñ–∏–∫–∞—Ü–∏—é...")
        
        prompt = DEAI_PROMPT.format(text=text)
        
        try:
            response = await self.ai.get_completion(
                prompt=prompt,
                temperature=0.5,
                max_tokens=1500,
                user_id=self.user_id,
                session_id=self.session_id,
                request_type="deai_processing"
            )
            
            if not response:
                logger.warning("‚ùå –ü–æ–ª—É—á–µ–Ω –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –ø—Ä–∏ –¥–µ–ò–ò-—Ñ–∏–∫–∞—Ü–∏–∏, –≤–æ–∑–≤—Ä–∞—â–∞—é –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç")
                return text
            
            logger.info("‚úÖ –î–µ–ò–ò-—Ñ–∏–∫–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
            return response.strip()
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–µ–ò–ò-—Ñ–∏–∫–∞—Ü–∏–∏: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç, –µ—Å–ª–∏ —á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫
            return text
    
    def _parse_json_safely(self, response: str, analysis_type: str) -> Dict[str, Any]:
        """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ JSON –æ—Ç–≤–µ—Ç–∞ –æ—Ç GPT"""
        try:
            # –û—á–∏—â–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç markdown –±–ª–æ–∫–æ–≤
            cleaned_response = response.strip()
            if cleaned_response.startswith('```json'):
                cleaned_response = cleaned_response[7:]  # —É–±–∏—Ä–∞–µ–º ```json
            if cleaned_response.endswith('```'):
                cleaned_response = cleaned_response[:-3]  # —É–±–∏—Ä–∞–µ–º ```
            cleaned_response = cleaned_response.strip()
            
            # –ü–∞—Ä—Å–∏–º JSON
            parsed = json.loads(cleaned_response)
            logger.info(f"‚úÖ JSON —É—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω –¥–ª—è {analysis_type}")
            return parsed
            
        except json.JSONDecodeError as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –¥–ª—è {analysis_type}: {e}")
            logger.error(f"–û—Ç–≤–µ—Ç GPT: {response[:500]}...")
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º fallback –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –∞–Ω–∞–ª–∏–∑–∞
            if analysis_type == "unified_analysis":
                return self._get_fallback_unified_analysis()
            elif analysis_type == "contextual_matching":
                return self._get_fallback_contextual_matching()
            else:
                return self._get_fallback_analysis()
    
    def _get_fallback_unified_analysis(self) -> Dict[str, Any]:
        """Fallback –¥–ª—è –µ–¥–∏–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
        return {
            "matches": [
                {
                    "requirement": "–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –æ–ø—ã—Ç",
                    "evidence": "–∏–º–µ–µ—Ç—Å—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –æ–ø—ã—Ç",
                    "quote": "–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç—Å—è —Ä–µ–∑—é–º–µ",
                    "strength": 0.7,
                    "explanation": "–±–∞–∑–æ–≤–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º"
                }
            ],
            "letter": "–£–≤–∞–∂–∞–µ–º—ã–µ –∫–æ–ª–ª–µ–≥–∏! –í–∞—à–∞ –≤–∞–∫–∞–Ω—Å–∏—è –ø—Ä–∏–≤–ª–µ–∫–ª–∞ –º–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ. –ú–æ–π –æ–ø—ã—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ—Å–Ω–æ–≤–Ω—ã–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º –ø–æ–∑–∏—Ü–∏–∏. –ì–æ—Ç–æ–≤ –æ–±—Å—É–¥–∏—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–∞.",
            "confidence": 0.6
        }
    
    def _get_fallback_contextual_matching(self) -> Dict[str, Any]:
        """Fallback –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π"""
        return {
            "matches": [
                {
                    "requirement": "–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–µ –Ω–∞–≤—ã–∫–∏",
                    "evidence": "—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –æ–ø—ã—Ç",
                    "quote": "–æ–ø–∏—Å–∞–Ω–æ –≤ —Ä–µ–∑—é–º–µ",
                    "strength": 0.7,
                    "type": "strong"
                }
            ],
            "gaps": []
        }

    def _get_fallback_analysis(self) -> Dict[str, Any]:
        """Fallback —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞ —Å–ª—É—á–∞–π –æ—à–∏–±–æ–∫ (—Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç)"""
        return {
            "vacancy_analysis": {
                "key_requirements": ["–û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã", "–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–µ –Ω–∞–≤—ã–∫–∏"],
                "pain_points": ["–ü–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç—å –≤ –∫–≤–∞–ª–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–º —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–µ"],
                "priorities": ["–ö–∞—á–µ—Å—Ç–≤–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á"],
                "company_culture": "–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è —Å—Ä–µ–¥–∞",
                "urgency_signals": [],
                "hidden_needs": ["–ù–∞–¥–µ–∂–Ω—ã–π –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å"]
            },
            "resume_analysis": {
                "key_skills": ["–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–µ –Ω–∞–≤—ã–∫–∏", "–û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã"],
                "achievements": ["–£—Å–ø–µ—à–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–¥–∞—á"],
                "unique_advantages": ["–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è —ç–∫—Å–ø–µ—Ä—Ç–∏–∑–∞"],
                "career_trajectory": "–°—Ç–∞–±–∏–ª—å–Ω–æ–µ —Ä–∞–∑–≤–∏—Ç–∏–µ",
                "transferable_skills": ["–ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ"]
            },
            "matching_strategy": {
                "direct_matches": ["–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º"],
                "skill_gaps": [],
                "positioning": "–ö–≤–∞–ª–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç",
                "value_proposition": "–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è —ç–∫—Å–ø–µ—Ä—Ç–∏–∑–∞ –∏ –æ–ø—ã—Ç",
                "specific_references": ["–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –æ–ø—ã—Ç —Ä–∞–±–æ—Ç—ã"]
            },
            "confidence_score": 0.6
        }

    # ============ LEGACY –ú–ï–¢–û–î–´ (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏) ============
    
    async def _legacy_generate_full_analysis_and_letter(
        self,
        vacancy_text: str,
        resume_text: str,
        style: str = "professional"
    ) -> Dict[str, Any]:
        """
        DEPRECATED: –°—Ç–∞—Ä—ã–π –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (–æ—Å—Ç–∞–≤–ª–µ–Ω –¥–ª—è fallback)
        """
        logger.warning("‚ö†Ô∏è DEPRECATED: –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è legacy –∞–ª–≥–æ—Ä–∏—Ç–º")
        
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π legacy –∞–ª–≥–æ—Ä–∏—Ç–º
            legacy_prompts = _get_legacy_prompts()
            
            # –û–¥–∏–Ω –∑–∞–ø—Ä–æ—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            analysis = await self._legacy_deep_analyze(vacancy_text, resume_text, legacy_prompts)
            
            # –û–¥–∏–Ω –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–∏—Å—å–º–∞
            letter = await self._legacy_generate_human_letter(analysis, style, legacy_prompts)
            
            # –î–µ-–ò–ò-—Ñ–∏–∫–∞—Ü–∏—è
            final_letter = await self.deai_text(letter)
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º legacy –∞–Ω–∞–ª–∏–∑
            formatted_analysis = self._format_legacy_analysis(analysis)
            
            return {
                'letter': final_letter,
                'analysis': formatted_analysis,
                'analysis_summary': formatted_analysis,
                'matches': [],  # Legacy –Ω–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç matches
                'stats': {
                    'total_tokens_used': self.total_tokens_used,
                    'models_used': list(self.models_used),
                    'generated_letter_length': len(final_letter),
                    'algorithm_version': 'legacy_v5.0'
                }
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ legacy –∞–ª–≥–æ—Ä–∏—Ç–º–∞: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            return {
                'letter': "–£–≤–∞–∂–∞–µ–º—ã–µ –∫–æ–ª–ª–µ–≥–∏! –†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Ä–∞–±–æ—Ç—ã –≤ –≤–∞—à–µ–π –∫–æ–º–ø–∞–Ω–∏–∏. –ú–æ–π –æ–ø—ã—Ç –∏ –Ω–∞–≤—ã–∫–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º –ø–æ–∑–∏—Ü–∏–∏. –ì–æ—Ç–æ–≤ –æ–±—Å—É–¥–∏—Ç—å –¥–µ—Ç–∞–ª–∏ —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–∞.",
                'analysis': "–ê–Ω–∞–ª–∏–∑ –≤—ã–ø–æ–ª–Ω–µ–Ω –≤ —É–ø—Ä–æ—â–µ–Ω–Ω–æ–º —Ä–µ–∂–∏–º–µ",
                'analysis_summary': "–ê–Ω–∞–ª–∏–∑ –≤—ã–ø–æ–ª–Ω–µ–Ω –≤ —É–ø—Ä–æ—â–µ–Ω–Ω–æ–º —Ä–µ–∂–∏–º–µ",
                'matches': [],
                'stats': {
                    'algorithm_version': 'fallback_minimal'
                }
            }
    
    async def _legacy_deep_analyze(self, vacancy_text: str, resume_text: str, prompts: dict) -> Dict[str, Any]:
        """Legacy –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑"""
        prompt = prompts['DEEP_ANALYSIS_PROMPT'].format(
            vacancy_text=vacancy_text,
            resume_text=resume_text
        )
        
        try:
            response = await self.ai.get_completion(
                prompt=prompt,
                temperature=0.3,
                max_tokens=2000,
                user_id=self.user_id,
                session_id=self.session_id,
                request_type="legacy_analysis"
            )
            
            if not response:
                return self._get_fallback_analysis()
            
            return self._parse_json_safely(response, "legacy_analysis")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ legacy –∞–Ω–∞–ª–∏–∑–∞: {e}")
            return self._get_fallback_analysis()
    
    async def _legacy_generate_human_letter(
        self, 
        analysis: Dict[str, Any], 
        style: str,
        prompts: dict
    ) -> str:
        """Legacy –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–∏—Å—å–º–∞"""
        prompt = prompts['HUMAN_WRITING_PROMPT'].format(
            analysis_json=json.dumps(analysis, ensure_ascii=False, indent=2),
            writing_style=style
        )
        
        try:
            response = await self.ai.get_completion(
                prompt=prompt,
                temperature=0.7,
                max_tokens=1500,
                user_id=self.user_id,
                session_id=self.session_id,
                request_type="legacy_letter_generation"
            )
            
            return response.strip() if response else "–ü–∏—Å—å–º–æ –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å"
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ legacy –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–∏—Å—å–º–∞: {e}")
            return "–ü–∏—Å—å–º–æ –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å"
    
    def _format_legacy_analysis(self, analysis: Dict[str, Any]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ legacy –∞–Ω–∞–ª–∏–∑–∞"""
        try:
            summary = "üìä –ê–ù–ê–õ–ò–ó –°–û–í–ü–ê–î–ï–ù–ò–ô (legacy)\n\n"
            
            # –û—Å–Ω–æ–≤–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
            matches = analysis.get('matching_strategy', {}).get('direct_matches', [])
            if matches:
                summary += "‚úÖ –ù–ê–ô–î–ï–ù–ù–´–ï –°–û–í–ü–ê–î–ï–ù–ò–Ø:\n"
                for i, match in enumerate(matches[:5], 1):
                    summary += f"{i}. {match}\n"
                summary += "\n"
            
            # –ü—Ä–æ–±–µ–ª—ã –≤ –Ω–∞–≤—ã–∫–∞—Ö
            gaps = analysis.get('matching_strategy', {}).get('skill_gaps', [])
            if gaps:
                summary += "‚ö†Ô∏è –û–ë–õ–ê–°–¢–ò –î–õ–Ø –†–ê–ó–í–ò–¢–ò–Ø:\n"
                for gap in gaps[:3]:
                    summary += f"‚Ä¢ {gap}\n"
                summary += "\n"
            
            # –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
            confidence = analysis.get('confidence_score', 0)
            summary += f"üìà –£–í–ï–†–ï–ù–ù–û–°–¢–¨: {confidence:.2f}\n"
            
            return summary
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è legacy –∞–Ω–∞–ª–∏–∑–∞: {e}")
            return "üìä Legacy –∞–Ω–∞–ª–∏–∑ –≤—ã–ø–æ–ª–Ω–µ–Ω"

    # Deprecated –º–µ—Ç–æ–¥—ã –¥–ª—è –ø–æ–ª–Ω–æ–π –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    async def deep_analyze(self, vacancy_text: str, resume_text: str) -> Dict[str, Any]:
        """DEPRECATED: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ analyze_and_generate_unified"""
        logger.warning("‚ö†Ô∏è DEPRECATED: deep_analyze —É—Å—Ç–∞—Ä–µ–ª, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ analyze_and_generate_unified")
        prompts = _get_legacy_prompts()
        return await self._legacy_deep_analyze(vacancy_text, resume_text, prompts)
    
    async def generate_human_letter(self, analysis: Dict[str, Any], style: str = "professional") -> str:
        """DEPRECATED: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ analyze_and_generate_unified"""
        logger.warning("‚ö†Ô∏è DEPRECATED: generate_human_letter —É—Å—Ç–∞—Ä–µ–ª, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ analyze_and_generate_unified")
        prompts = _get_legacy_prompts()
        return await self._legacy_generate_human_letter(analysis, style, prompts)
    
    async def generate_full_letter(self, vacancy_text: str, resume_text: str, style: str = "professional") -> str:
        """DEPRECATED: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ analyze_and_generate_unified"""
        logger.warning("‚ö†Ô∏è DEPRECATED: generate_full_letter —É—Å—Ç–∞—Ä–µ–ª, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ analyze_and_generate_unified")
        result = await self._legacy_generate_full_analysis_and_letter(vacancy_text, resume_text, style)
        return result['letter']
    
    async def generate_full_analysis_and_letter(self, vacancy_text: str, resume_text: str, style: str = "professional") -> Dict[str, Any]:
        """DEPRECATED: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ analyze_and_generate_unified"""
        logger.warning("‚ö†Ô∏è DEPRECATED: generate_full_analysis_and_letter —É—Å—Ç–∞—Ä–µ–ª, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ analyze_and_generate_unified")
        return await self._legacy_generate_full_analysis_and_letter(vacancy_text, resume_text, style)


# ============ –ì–õ–û–ë–ê–õ–¨–ù–´–ô –≠–ö–ó–ï–ú–ü–õ–Ø–† ============

_analyzer_instance: Optional[SmartAnalyzer] = None


def get_analyzer(ai_service=None) -> SmartAnalyzer:
    """–ü–æ–ª—É—á–∏—Ç—å –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
    global _analyzer_instance
    
    if _analyzer_instance is None:
        if ai_service is None:
            from services.ai_factory import get_ai_service
            ai_service = get_ai_service()
        _analyzer_instance = SmartAnalyzer(ai_service)
    
    return _analyzer_instance


async def analyze_and_generate(
    vacancy_text: str,
    resume_text: str,
    style: str = "professional",
    ai_service=None
) -> str:
    """
    DEPRECATED: –£–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ —Ñ–ª–æ—É (–æ–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å)
    –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ analyze_and_generate_with_analysis –¥–ª—è –Ω–æ–≤—ã—Ö –ø—Ä–æ–µ–∫—Ç–æ–≤
    """
    logger.warning("‚ö†Ô∏è DEPRECATED: analyze_and_generate —É—Å—Ç–∞—Ä–µ–ª, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ analyze_and_generate_with_analysis")
    analyzer = get_analyzer(ai_service)
    result = await analyzer._legacy_generate_full_analysis_and_letter(vacancy_text, resume_text, style)
    return result['letter']


async def analyze_and_generate_with_analysis(
    vacancy_text: str,
    resume_text: str,
    style: str = "professional",
    ai_service=None,
    user_id: Optional[int] = None,
    session_id: Optional[str] = None
) -> Dict[str, Any]:
    """
    üöÄ –û–°–ù–û–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø v6.0: –ê–Ω–∞–ª–∏–∑ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
    
    Args:
        vacancy_text: –¢–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏
        resume_text: –¢–µ–∫—Å—Ç —Ä–µ–∑—é–º–µ  
        style: –°—Ç–∏–ª—å –ø–∏—Å—å–º–∞
        openai_service: –°–µ—Ä–≤–∏—Å OpenAI (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏
        session_id: ID —Å–µ—Å—Å–∏–∏ –¥–ª—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏
        
    Returns:
        Dict —Å –ø–∏—Å—å–º–æ–º –∏ –∞–Ω–∞–ª–∏–∑–æ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
    """
    analyzer = get_analyzer(ai_service)
    # –ü–µ—Ä–µ–¥–∞–µ–º –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
    analyzer.user_id = user_id
    analyzer.session_id = session_id
    
    # A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ: –≤—ã–±–æ—Ä –∞–ª–≥–æ—Ä–∏—Ç–º–∞
    use_unified = os.getenv('USE_UNIFIED_ANALYSIS', 'true').lower() == 'true'
    
    if use_unified:
        logger.info("üöÄ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º v6.0 (–µ–¥–∏–Ω—ã–π –∞–Ω–∞–ª–∏–∑)")
        return await analyzer.analyze_and_generate_unified(vacancy_text, resume_text, style)
    else:
        logger.info("üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è legacy –∞–ª–≥–æ—Ä–∏—Ç–º v5.0 (–º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω—ã–π)")
        return await analyzer._legacy_generate_full_analysis_and_letter(vacancy_text, resume_text, style) 